---
layout: post
author: Tim Davies
title: Nine considerations for effective global deliberation on AI
projects: 
category: blog
---

**What could global deliberation on AI governance look like?** And what can we learn from this for thinking about collective and participatory data governance?

<!--more-->

On Wednesday last week, Richard Bartlett, co-founder of collective decision making platform Loomio, posted [a simple tweet](https://twitter.com/RichDecibels/status/1641118020918558725):

> *so OpenAI CEO @sama said his dream scenario for how to align AI is to run a global deliberative democracy process to define the boundaries of the system and ah... I have friends who know how to do that. who can I talk to make this more likely to happen?*

In sparked quite the thread, and yesterday, thanks to a few local [Stroudie](https://en.wikipedia.org/wiki/Stroud#) connections, I found myself in a conversation that picked up on that message, with a rather inspiring group of thinkers and doers of deliberative and digital democracy exploring the what, and the how, of global AI governance deliberation.

In this post, I've tried to capture some of my reflections from that call, in the form of nine elements that may be important to consider for both a pragmatic, and principled, approach to global deliberations on AI in the hope both that these are useful to the collaborators from yesterday's call, and for my own thinking aloud about how these ideas might also be applied to broader data governance deliberations.


### Setting the context: AI, a pause, and the possibility of global deliberation

The last couple of months have seen _a lot_ of news, excitement, and fear, about 'AI'. This has been particularly centred on the release and rapid adoption of tools like ChatGPT and image generators, and a race to develop and integrate increasingly convincing [generative AI systems](https://en.wikipedia.org/wiki/Generative_artificial_intelligence).

Eight days ago, Sam Altman, OpenAI CEO, discussed in an interview the challenges large firms face in deciding what increasingly powerful Large Language Models (LLMs) should or should not do, and [raised the potential for a global deliberative conversation on systems like GPT-4](https://www.youtube.com/clip/UgkxOU5gofiwebESRMqKCL2CvVIJg9upewzs) (the language model behind tools like ChatGPT):

> "My dream scenario, and I don't think we can quite get here, but like let's say this is the platonic ideal... is that every person on Earth would come together to have a really thoughtful deliberative conversation about where we want to draw the boundary on this system and we would have something like the U.S. Constitutional Convention where we debate the issues, and look at things from different perspectives.... and then we agree on, like here are the rules, here are the overall rules of this system, and it was a democratic process."

This came in the wake of an Open Letter from many 'big names' in machine-learning development, coordinated by the [longtermist](https://en.wikipedia.org/wiki/Longtermism) Future of Life Institute, calling for a [6-month pause in 'giant AI experiments'](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), pointing to both 'existential risk' concerns, and giving society 'time to adapt'. This raises the question: how might a public conversation over the course of six months be able to shape the next stages of AI research? That letter has been [met with a rigorous reply from influential researchers at the Distributed AI Research Institute (DAIR)](https://www.dair-institute.org/blog/letter-statement-March2023) who point out that the harms of AI are not hypothetical or future, but experienced now, and particularly by marginalised communities. This centres the debate not on improving how the outputs of machine-learning systems align with public values, but rather places emphasis on the regulation of the economic and technological contexts in which they operate.

Working at [Connected by Data](https://connectedbydata.org/), with our focus on giving communities a powerful voice in data governance, I see overlaps between AI governance and data governance. After all, many AI governance questions are _also_ data governance questions. At the same time, there are risks that a focus on 'cutting edge' AI shifts attention away from broader governance of more 'mundane', but equally impactful, non-AI data practices and their social impacts.

It's in this context that I joined yesterday's discussion, keen to also learn from other participants on the call who have variously theorised and organised nationwide and global deliberations on pressing policy issues: hearing from [Audrey Tang](https://en.wikipedia.org/wiki/Audrey_Tang) on their work using [Pol.is](https://pol.is/) and other tools in Taiwan, and [Diviya Siddarth](https://divyasiddarth.com/) and [Saffron Huang's](https://saffronhuang.com/) work exploring at-scale aggregation of views in the [Collective Intelligence Project](https://cip.org/), to [Claire Melier](https://www.clairemellier.com/), [Richard Wilson](https://uk.linkedin.com/in/richwi1son) and colleagues work on the [Global Climate Assembly](https://globalassembly.org/).

Over the course of an hour we explored emerging proposals for rapid 'Alignment Assemblies' that might feed directly into the work of leading Silicon Valley firms in training of generative AI models, and the possibility of 'agile governance sprints' that might operate alongside software development cycles to rapidly iterate on models of broader and more diverse, deliberative or 'democratic' input and decision making. We also discussed the need to keep the broader impacts of AI adoption in focus: supporting national and cross-cultural dialogue that considers 'mitigation, adaptation and resilience' to the social and economic impacts of AI (to borrow a framing from Climate Change discussions), and the possibility of distributed dialogues that not only feed into national and global policy-making, but that also build the capacity of communities to shape more local technology adoption decisions.

Broadly the sense from the discussion was that, even if Sam Altman's intervention was not an entirely serious call for global deliberation on AI, such deliberation and debate is not only desirable, but is eminently possible. So what might it take to be effective? Here's nine partial thoughts reflecting on that question.


### Element 1: Start from decisions and work backwards

The design of a large-scale deliberation faces a couple of challenges: (1) who should participate; and (2) how should inputs from multiple participants be aggregated and presented.

Key to answering these questions is understanding the decisions that a process seeks to influence.

For example, a process that seeks to feed directly into decisions during AI model development may benefit from open large-scale participation in which inputs can be transparently aggregated through platforms like [pol.is](https://pol.is/home) that offer insight into both majority views, and outlier community needs that models need to take into account. The exact models of opinion aggregation in such cases may need to be tightly tailored, at least in the short-term, to the capability of AI models to incorporate such inputs.

By contrast, a process oriented towards shaping government policy decisions may need to employ a clear [sortition model](https://en.wikipedia.org/wiki/Sortition) to recruit a random representative sample of participants, and then to create layers of report-drafting to produce consensus recommendations for action that command broad support.

It may be possible for a global deliberation to have shared starting points that feed into different aggregation routes. For example, a series of distributed dialogues might take place in different contexts across the world to support education and opinion formation. These could then both feed views and conclusions from dialogue participants into a pol.is like platform for a statistical aggregation of views, as well as acting as the starting constituency for a sortition base selection of participants to join a global dialogue. 


### Element 2: Foreground power

A much-made, but far-too-often neglected point. As Catherine D'Ignazio and Lauren Klein [remind us in the opening of Data Feminism](https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4), our first step should always be to _examine power_.

In the AI context, right now a number of large firms appear to hold significant power. At the same time, governments are exploring the regulatory power they may have to address the benefits and harms of AI, and different actors are looking at what power the law may have to enable or constrain aspects of AI development and use.

A commitment to democracy involves considering how far a deliberation works towards a more equitable distribution of power, or how much it may support greater power concentration. For example, a deliberation that feeds primarily into AI model development, but that leaves questions about the broader governance of large AI systems untouched, may further a centralisation of power with AI firms, rather than helping to bring these technologies within the ambit of democratic control.


### Element 3: Trustworthy process (meta-governance)

This leads to the critical point, that a credible global deliberation needs to be trustworthy. However, responding to the call or opportunity for a rapid process means it's likely to be impossible to get everything right the first time, and the difference in resourcing of venture-capital backed firms, as against civil society organisations and governments, presents a high risk of process capture.

For that reason, I'd argue there will be a substantial need for meta-governance: advisory, oversight and scrutiny processes that can monitor any processes, provide challenge, help them to improve over time, and call-out any critical problems.

(As we've seen in a lot of the participatory data governance processes we've been exploring at Connected by Data, an effective oversight group for a process can be vitally important to ensuring both quality and credibility of engagement.)


### Element 4: Address polarisation

Debates about AI bring into sharp relief the challenges of polarisation. Not only are participants likely to approach deliberation with different political worldviews (capitalist, socialist, individualist, communitarian, western, southern, eastern etc.), but the debates around AI itself include some significant ideological positions and commitments: from those who anticipate the emergence of genuine 'Artificial General Intelligence (AGI)', to those who see this as hype and science-fiction, distracting from the present material consequences of algorithmic technology.

Deliberative democracy has long been positioned by leading proponents such as [Fishkin as a tool to reduce polarisation](https://www.cambridge.org/core/journals/american-political-science-review/article/is-deliberation-an-antidote-to-extreme-partisan-polarization-reflections-on-america-in-one-room/5DEFB6F8D944ECDE77A5E80C3346D4DE), although [others have questioned how far this effect holds](https://researchportal.vub.be/en/publications/deliberation-and-polarization-a-multi-disciplinary-review), citing in particular the importance of face-to-face engagement in creating depolarisation effects. Critically, the design of inputs and facilitation are important components of how far processes can address polarisation, and a global deliberation playing out in online spaces that tend to amplify, rather than ameliorate, disagreement, raises particular challenges.

In practical terms, there is an opportunity for a global deliberation to create robust and non-polemic materials that clearly articulate different viewpoints on AI, as a starting point to then understanding and, hopefully addressing, some of the polarisation currently present.


### Element 5: Seek to transform decision makers

In our recent [evaluation of a deliberative engagement on use of digitised court records](http://connectedbydata.org/resources/justice-data-matters-2022-evaluation-report) we explored the extent to which _direct experience_ of a deliberative process can play an important role in changing the attitudes, assumptions and ultimately actions, of decision-makers. In this sense, perhaps one of the important opportunities of a deliberative conversation on AI is the chance for key industry and government AI leaders to have a positive, open and depolarising experience of different attitudes towards AI.

In practice, this could mean paying considerable attention to the mode in which these actors might observe or engage in deliberations, and designing processes that particularly address the power imbalances created by the technical subject matter.

In other words, it may be important, for example, for key engineers to have positive experiences of hearing public voices on AI, without feeling that the views they are hearing are uninformed, or irrelevant to the direct problems of model development or governance. Bridging technical and non-technical '[cultures](https://academic.oup.com/book/8367/chapter-abstract/154049165?redirectedFrom=fulltext)' requires intentional facilitation.


### Element 6: Bridge conversations

Within a global deliberation it will also be important to find ways to bridge between different conversations. The concerns in one area of the world, or in one community, may not be the same as those in others. On Monday's call [Yago Bermejo Abati](https://twitter.com/iacocoba) highlighted the challenges of holding cross-lingual conversations and deliberation, and the time and effort this takes. While technologies (and particularly Large Language Models as it happens) may have a useful role to play in summarising content from different conversations, and finding connections or differences, machine translation alone is inadequate (and at this stage of the debate, lacks the legitimacy) to bridge conversations between cultures and jurisdictions and across diverse contexts.

In an agile process, it may be important to set many local conversations in train, and then work out over time how to connect them - but a focus on how these connections and bridges may be built should not be neglected.


### Element 7: Prioritise affected communities and present problems

If you only have the resources to start deliberation on the governance of technology in one or two settings, where should you prioritise? It can be tempting to focus on the places closest to home: the local communities around where the tech is developed for example.

In [Connected by Data](https://connectedbydata.org/)'s work, we're trying to explore instead how any deliberation should start from identifying affected communities, and prioritising their engagement.

Linked to this is the idea that the content and focus of particular deliberation should start from where people are at: from the problems communities face, and the problems data or technology systems are trying to solve. (This need not be in conflict with #1: process design starts from decisions; process recruitment and content from those most affected, and the most present issues.)


### Element 8: Build on, and into, existing  governance

A few days back UNESCO [gently pointed out](https://www.unesco.org/en/articles/artificial-intelligence-unesco-calls-all-governments-implement-global-ethical-framework-without) that they published a comprehensive ethical framework for AI in 2021, with the backing of all 193 member states. AI governance will undoubtedly be on the agenda of many global processes this year, and not for the first time. While on the one-hand, participatory and deliberative processes emerge to deal with the limitations of current political and governance structures, it is important to consider how they interface with them. It is at least worthwhile to give some consideration to how deliberative processes might feed into existing experiments with multi-stakeholder governance such as the [Internet Governance Forum](https://www.intgovforum.org/) and [Open Government Partnership](https://www.opengovpartnership.org/), and to be part of strengthening democratic practice. 


### Element 9: Recognise multiple pathways to impact

This point I owe to Richard Wilson's presentation of [the IZWE Theory of Change](https://docs.google.com/document/d/1uXzuzGC4M1fMfZtuNxMdGIvvUv5igTEgCSmWOem8-8Q/edit), and [Gaventa's Power Cube](https://www.powercube.net/wp-content/uploads/2009/12/finding_spaces_for_change.pdf) - highlighting the multiple levels, spaces and forms of power that could be addressed by an effective deliberation on AI governance.

In short, a deliberative process has the potential not only to feed into some global decision, but also to forge new relationships, and educate and activate citizens to act on issues within their local contexts. From the Connected by Data perspective this is particularly important: AI and data governance is not just about setting global rules, or shaping large-scale systems - it is also about helping communities develop voice and power to have a say over how data and AI are adopted in, and impact on, local settings such as schools, workplaces, and local public services.

Designing deliberation with these other pathways to impact in mind both makes a process more resilient, and makes it more likely that it will contribute to addressing power in more meaningful ways.


### From AI to data governance in general

The elements above are neither groundbreaking, or exhaustive. They draw on learning from many people and many decades of participation practice (and apologies where I've missed attribution on ideas: happy to amend and add links). However, I hope they are useful in the context of a current rush to respond to the governance vacuum around AI, and that they start to point to wider issues that might need to feature in the design of different participatory data governance processes.

Over the coming months at Connected by Data we're going to be exploring ways of supporting data governance process design. If you'd like to find out more, [do get in touch](mailto:tim@connectedbydata.org).

*Thanks to Jeni Tennison and Maria Luciano for comments on the draft of this post.*
