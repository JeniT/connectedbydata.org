---
layout: post
author: Maria Luciano
category: weeknotes
---

It’s been two months since I joined Connected by Data. As someone who is fueled by learning new things, this experience has allowed me to reflect and expand on topics that interest me and are also essential to data discussions: power, storytelling, and community building.

Last month I stumbled upon Professor Orly Lobel’s piece “[Why your privacy shouldn’t be so private](https://thehill.com/opinion/technology/3761229-why-your-privacy-shouldnt-be-so-private/).” Her main argument there, that collecting more data could lead to “more diverse” datasets and more effective policies, stuck with me. It reminded me of our discussion at Alan Turing Institute’s “[AI Ethics and Governance](https://alan-turing-institute.github.io/turing-commons/aeg/)” course in November. Following Abeba Birhane”s brilliant lecture, we talked about some misconceptions surrounding the notion of “bias” and how it assumes the existence of a ‘correct’ view of the world. It misses the mark in acknowledging that technology only picks up patterns that already exist in society - created by “[contextual and historical norms and structures](https://www.sciencedirect.com/science/article/pii/S2666389921000155).” 

Last week Professor Orly Lobel published her new paper, “[The Law of AI for Good](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4338862),” expanding on her ideas. She challenges the pessimistic views around technology and its consequential risk-based regulation, which seem to overlook the benefits that technology can offer and the fact that human decision-making also poses risks to unfair decisions. She cites an example of an [Amazon hiring algorithm](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G) that was tested for bias, deemed sexist as it picked up hiring patterns already in place, and never deployed by the company - which the author seems to praise. But Amazon was also the company whose first response to the Gender Shades research results was to discredit its principal investigators and [undermine](https://www.technologyreview.com/2020/06/12/1003482/amazon-stopped-selling-police-face-recognition-fight/) them as Black women. 

Approaching data governance through a collective lens allows us to think about the development of these technologies from the beginning: who are the people calling the shots? The idea that “the more data, the better” doesn’t seem to address concerns around the power imbalance between companies, governments, and citizens, as well as the opacity of the decision-making processes regarding data usage of the first two. It seems to miss the fact that some groups have been historically excluded from those decisions and conversations, which gives them “[the epistemic authority](https://www.sciencedirect.com/science/article/pii/S2666389921000155) to recognize injustice and harm given their lived experience” and makes questionable the legitimacy of current decisions such as “which benefits we should aim for” and “which risks are we looking to take to get there.”

These reflections shed light on the case studies database we have worked on. They illustrate the importance of storytelling, acknowledging narratives, and engaging with specific audiences for specific purposes. Topics that can be overlooked in academia usually focus on a more structured form of presenting data. Thinking about approaching these aspects in our database will be the adventure of the next couple of weeks.
